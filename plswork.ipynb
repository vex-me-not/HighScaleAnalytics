{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and the test datasets...\n",
      "Splitting the training dataset to a smaller one ...\n",
      "Splitting the testing dataset to a smaller one ...\n",
      "Performing preprocessing and shingling of the data...\n",
      "Shingling the training set...\n",
      "Shingling the test set...\n",
      "Performing Brute-Force K-NN...\n",
      "Brute-Force K-NN completed in 71.98 seconds.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from itertools import zip_longest\n",
    "from time import time\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from scipy.sparse import csr_matrix\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# Part 2: Nearest Neighbor Search with Locality Sensitive Hashing (LSH)\n",
    "def set_to_list(list):\n",
    "    s=set()\n",
    "    for item in list:\n",
    "        s.add(item)\n",
    "    return s\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing and removing punctuation.\n",
    "    :param text: The input text string.\n",
    "    :return: Preprocessed text string.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def text_to_shingles(text, k=4):\n",
    "    \"\"\"\n",
    "    Convert text into shingles (sets of overlapping word sequences of size k).\n",
    "    :param text: Input string to be converted into shingles.\n",
    "    :param k: Shingle size (default is 5).\n",
    "    :return: A set of k-length shingles.\n",
    "    \"\"\"\n",
    "    text = preprocess_text(text)  # Preprocess the text\n",
    "    # word_list = text.split()\n",
    "    # shingles = zip_longest(*[words[i:] for i in range(k)], fillvalue='')\n",
    "    # Filter out incomplete shingles containing the fillvalue\n",
    "    # text= re.sub(r'[,;.:?!@*&#-_|\\s]','',text)\n",
    "    shingles = []\n",
    "    for i in range(0, len(text) - k):\n",
    "        shingles.append(text[i:i + k])\n",
    "    return set(shingles)\n",
    "    # return [word for word in word_list if word not in stopwords.words('english')]\n",
    "    # return {shingle for shingle in shingles if '' not in shingle}\n",
    "\n",
    "def create_minhash_vectorized(text_series, num_perm):\n",
    "    \"\"\"\n",
    "    Create MinHash objects for a series of texts using shingles.\n",
    "    :param text_series: Pandas Series of input strings to hash.\n",
    "    :param num_perm: Number of permutations for MinHash.\n",
    "    :return: List of MinHash objects.\n",
    "    \"\"\"\n",
    "    minhash_list = []\n",
    "    for text in text_series:\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for shingle in text_to_shingles(text):\n",
    "            m.update(''.join(shingle).encode('utf-8'))\n",
    "        minhash_list.append(m)\n",
    "    return minhash_list\n",
    "\n",
    "def jaccard_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute Jaccard distance between two sparse vectors/matrices.\n",
    "    :vector1: First sparse vector, containing float numbers.\n",
    "    :vector2: Second sparse vector, containing float numbers\n",
    "    :return: Jaccard distance score(1-Jaccard similarity).\n",
    "    \"\"\"\n",
    "    # we make the vectors/matrices dense so that we can work on them\n",
    "    v1dense=vector1.todense()\n",
    "    v2dense=vector2.todense()\n",
    "    \n",
    "    # we turn them into boolean vectors, in which every value is either True or False\n",
    "    v1bool=v1dense.astype(bool)\n",
    "    v2bool=v2dense.astype(bool)\n",
    "\n",
    "    # The intersection of the two boolean vectors is the result of the logical AND\n",
    "    intersection = np.logical_and(v1bool,v2bool) \n",
    "    \n",
    "    # The union of the two boolean vectors is the result of the logical OR\n",
    "    union = np.logical_or(v1bool,v2bool)\n",
    "    \n",
    "    # The size of the intersection is the number of all the non-Zero(non-False) items\n",
    "    intersection_size=np.count_nonzero(intersection)\n",
    "    # The size of the union is the number of all the non-Zero(non-False) items\n",
    "    union_size=np.count_nonzero(union)\n",
    "\n",
    "    return (1- intersection_size / union_size) if union_size > 0 else 0\n",
    "\n",
    "\n",
    "def jaccard_similarity(doc1: set,doc2:set):\n",
    "    intersection = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "\n",
    "    return 1 - len(intersection)/len(union)\n",
    "\n",
    "\n",
    "def create_minhash(vector, num_perm):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "    # print(\"Create minhash vector\",vector)\n",
    "    for idx in vector:\n",
    "        print(idx)\n",
    "        m.update(str(idx).encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def nearest_neighbor_search():\n",
    "    # Load datasets\n",
    "    print(\"Loading the training and the test datasets...\")\n",
    "    train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "    test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "    fraction=0.01 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "    # Split the training dataset into a smaller one\n",
    "    print(\"Splitting the training dataset to a smaller one ...\")\n",
    "    train_subset, _ = train_test_split(\n",
    "        train_ds_comma, \n",
    "        train_size=fraction, \n",
    "        stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "        random_state=42 \n",
    "    )\n",
    "\n",
    "    train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "    # Split the testing dataset into a smaller one\n",
    "    print(\"Splitting the testing dataset to a smaller one ...\")\n",
    "    test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "    test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "    # the two sets that we are going to work on\n",
    "    train_subset=pd.read_csv('train_subset.csv')\n",
    "    test_subset=pd.read_csv('test_subset.csv')  \n",
    "\n",
    "    train_df = train_subset\n",
    "    test_df = test_subset\n",
    "\n",
    "    # Example dataset\n",
    "    train_docs = train_df['Content'] + train_df['Title']\n",
    "    test_docs = test_df['Content'] + test_df['Title']\n",
    "\n",
    "    # Preprocessing of the text data\n",
    "    print(\"Performing preprocessing and shingling of the data...\")\n",
    "\n",
    "    print(\"Shingling the training set...\")\n",
    "    list_of_train_shingle=[]\n",
    "    for tr_doc in train_docs:\n",
    "        # print(\"OG text is:\",tr_doc)\n",
    "        sh_doc=text_to_shingles(tr_doc)\n",
    "        # print(\"Shingled  text is:\",set(sh_doc))\n",
    "        list_of_train_shingle.append(sh_doc)\n",
    "\n",
    "    print(\"Shingling the test set...\")\n",
    "    list_of_test_shingle=[]\n",
    "    for te_doc in test_docs:\n",
    "        # print(\"OG text is:\",tr_doc)\n",
    "        sh_doc=text_to_shingles(te_doc)\n",
    "        # print(\"Shingled  text is:\",set(sh_doc))\n",
    "        list_of_test_shingle.append(sh_doc)\n",
    "\n",
    "    # for i in range(len(list_of_train_shingle)): \n",
    "    #     print(list_of_train_shingle[i])\n",
    "\n",
    "    # for i in range(len(list_of_test_shingle)): \n",
    "    #     print(list_of_test_shingle[i])\n",
    "    \n",
    "\n",
    "    # Compute K=7 nearest neighbors for each test document\n",
    "    K = 7\n",
    "    nearest_neighbors = []\n",
    "    \n",
    "    # Brute-force K-NN\n",
    "    print(\"Performing Brute-Force K-NN...\")\n",
    "    start_time = time() # we start the clock\n",
    "    \n",
    "    for teindex in range(len(list_of_test_shingle)):\n",
    "        similarities = []\n",
    "        \n",
    "        for traindex in range(len(list_of_train_shingle)):\n",
    "            sim = jaccard_similarity(list_of_test_shingle[teindex], list_of_train_shingle[traindex])\n",
    "            similarities.append((traindex, sim)) # we keep indexes of train sets\n",
    "        \n",
    "        # print(\"Similarities are\",similarities)\n",
    "        sorted_sim=sorted(similarities,key=lambda x : x[1])\n",
    "        # print(\"Sorted similiraties\",sorted_sim)\n",
    "        top_k_neighbors = [idx for idx, sim in sorted_sim[:K]]\n",
    "        \n",
    "        nearest_neighbors.append(top_k_neighbors)\n",
    "    \n",
    "    brute_force_time = time() - start_time # we stop the clock\n",
    "\n",
    "    # Print the K=7 nearest neighbors for each test document\n",
    "    # for test_doc, neighbors in enumerate(nearest_neighbors):\n",
    "    #     print(f\"Test Document {test_doc}: Nearest Neighbors (Train Indexes): {neighbors}\")\n",
    "\n",
    "\n",
    "\n",
    "    print(f\"Brute-Force K-NN completed in {brute_force_time:.2f} seconds.\")\n",
    "\n",
    "        # test_df['Predicted_BruteForce'] = brute_force_predictions\n",
    "        # test_df[['Id', 'Predicted_BruteForce']].to_csv('brute_force_knn_predictions.csv', index=False)\n",
    "\n",
    "    return\n",
    "    # LSH-based K-NN\n",
    "    print(\"Performing LSH-based K-NN...\")\n",
    "    for num_perm in [16,32,64]:\n",
    "        start_time = time()\n",
    "        lsh = MinHashLSH(threshold=0.1, num_perm=num_perm)\n",
    "        train_minhashes = [create_minhash(X_train[i], num_perm) for i in range(X_train.shape[0])]\n",
    "        \n",
    "        for i, minhash in enumerate(train_minhashes):\n",
    "            lsh.insert(str(i), minhash)\n",
    "        \n",
    "        build_time = time() - start_time\n",
    "        start_time = time()\n",
    "        lsh_results = []\n",
    "        \n",
    "        for i in range(X_test.shape[0]):\n",
    "            test_minhash = create_minhash(X_test[i], num_perm)\n",
    "            candidates = lsh.query(test_minhash)\n",
    "            #print(candidates)\n",
    "            lsh_results.append([int(c) for c in candidates])\n",
    "        \n",
    "        query_time = time() - start_time\n",
    "        \n",
    "        matched_fractions = []\n",
    "        for brute_neighbors, lsh_neighbors in zip(nearest_neighbors, lsh_results):\n",
    "            matched_count = len(set(brute_neighbors) & set(lsh_neighbors))\n",
    "            matched_fraction = matched_count / len(brute_neighbors) if brute_neighbors else 0\n",
    "            matched_fractions.append(matched_fraction)\n",
    "        \n",
    "        average_fraction = np.mean(matched_fractions) if matched_fractions else 0\n",
    "        \n",
    "        print(f\"LSH Results (num_perm={num_perm}):\")\n",
    "        print(f\"  Build Time: {build_time:.2f} seconds\")\n",
    "        print(f\"  Query Time: {query_time:.2f} seconds\")\n",
    "        print(f\"  Fraction Matched: {average_fraction:.2f}\")\n",
    "if __name__ == \"__main__\":\n",
    "    nearest_neighbor_search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
