{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training and the test datasets...\n",
      "Splitting the training dataset to a smaller one (fraction:10.0%) ...\n",
      "Splitting the testing dataset to a smaller one (fraction:10.0%) ...\n",
      "Performing preprocessing and shingling of the data...\n",
      "Shingling the training set... (k=3)\n",
      "Shingling the test set... (k=3)\n",
      "Performing Brute-Force K-NN...\n",
      "Brute-Force K-NN completed in 5544.29 seconds.\n",
      "Performing LSH-based K-NN...\n",
      "LSH Results (num_perm=16,threshold=0.4):\n",
      "  Build Time: 91.79 seconds\n",
      "  Query Time: 44.16 seconds\n",
      "  Fraction Matched: 0.43\n",
      "LSH Results (num_perm=32,threshold=0.4):\n",
      "  Build Time: 94.69 seconds\n",
      "  Query Time: 43.36 seconds\n",
      "  Fraction Matched: 0.33\n",
      "LSH Results (num_perm=64,threshold=0.4):\n",
      "  Build Time: 99.11 seconds\n",
      "  Query Time: 43.26 seconds\n",
      "  Fraction Matched: 0.17\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "# Part 2: Nearest Neighbor Search with Locality Sensitive Hashing (LSH)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing and removing punctuation.\n",
    "    :param text: The input text string.\n",
    "    :return: Preprocessed text string.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub(r'[^\\w]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def text_to_shingles(text, k=3):\n",
    "    \"\"\"\n",
    "    Convert text into shingles (sets of overlapping word sequences of size k).\n",
    "    :param text: Input string to be converted into shingles.\n",
    "    :param k: Shingle size (default is 5).\n",
    "    :return: A set of k-length shingles.\n",
    "    \"\"\"\n",
    "    text = preprocess_text(text)  # Preprocess the text\n",
    "\n",
    "    shingles = []\n",
    "    for i in range(0, len(text) - k):\n",
    "        shingles.append(text[i:i + k])\n",
    "    return set(shingles)\n",
    "    # return [word for word in word_list if word not in stopwords.words('english')]\n",
    "\n",
    "\n",
    "def jaccard_similarity(doc1: set,doc2:set):\n",
    "    intersection = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "\n",
    "    return 1 - len(intersection)/len(union)\n",
    "\n",
    "\n",
    "def create_minhash(shingle_doc, num_perm):\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "\n",
    "    for shingle in shingle_doc:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "    return m\n",
    "\n",
    "def nearest_neighbor_search():\n",
    "    # Load datasets\n",
    "    print(\"Loading the training and the test datasets...\")\n",
    "    train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "    test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "    fraction=0.10 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "    # Split the training dataset into a smaller one\n",
    "    print(f\"Splitting the training dataset to a smaller one (fraction:{fraction*100}%) ...\")\n",
    "    train_subset, _ = train_test_split(\n",
    "        train_ds_comma, \n",
    "        train_size=fraction, \n",
    "        stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "        random_state=42 \n",
    "    )\n",
    "\n",
    "    train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "    # Split the testing dataset into a smaller one\n",
    "    print(f\"Splitting the testing dataset to a smaller one (fraction:{fraction*100}%) ...\")\n",
    "    test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "    test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "    # the two sets that we are going to work on\n",
    "    train_subset=pd.read_csv('train_subset.csv')\n",
    "    test_subset=pd.read_csv('test_subset.csv')  \n",
    "\n",
    "    train_df = train_subset\n",
    "    test_df = test_subset\n",
    "\n",
    "    # Example dataset\n",
    "    train_docs = train_df['Content'] + train_df['Title']\n",
    "    test_docs = test_df['Content'] + test_df['Title']\n",
    "\n",
    "    # Preprocessing of the text data\n",
    "    print(\"Performing preprocessing and shingling of the data...\")\n",
    "\n",
    "    k_shingle=3\n",
    "\n",
    "    print(f\"Shingling the training set... (k={k_shingle})\")\n",
    "    list_of_train_shingle=[]\n",
    "    for tr_doc in train_docs:\n",
    "        sh_doc=text_to_shingles(tr_doc,k=k_shingle)\n",
    "        list_of_train_shingle.append(sh_doc)\n",
    "\n",
    "    print(f\"Shingling the test set... (k={k_shingle})\")\n",
    "\n",
    "    list_of_test_shingle=[]\n",
    "    for te_doc in test_docs:\n",
    "        sh_doc=text_to_shingles(te_doc,k=k_shingle)\n",
    "        list_of_test_shingle.append(sh_doc)\n",
    "\n",
    "    \n",
    "    # Compute K=7 nearest neighbors for each test document\n",
    "    K = 7\n",
    "    nearest_neighbors = []\n",
    "    \n",
    "    # Brute-force K-NN\n",
    "    print(\"Performing Brute-Force K-NN...\")\n",
    "    start_time = time() # we start the clock\n",
    "    \n",
    "    for teindex in range(len(list_of_test_shingle)):\n",
    "        similarities = []\n",
    "        \n",
    "        for traindex in range(len(list_of_train_shingle)):\n",
    "            sim = jaccard_similarity(list_of_test_shingle[teindex], list_of_train_shingle[traindex])\n",
    "            similarities.append((traindex, sim)) # we keep indexes of train sets\n",
    "        \n",
    "        sorted_sim=sorted(similarities,key=lambda x : x[1])\n",
    "        top_k_neighbors = [idx for idx, sim in sorted_sim[:K]]\n",
    "        \n",
    "        nearest_neighbors.append(top_k_neighbors)\n",
    "    \n",
    "    brute_force_time = time() - start_time # we stop the clock\n",
    "\n",
    "\n",
    "    print(f\"Brute-Force K-NN completed in {brute_force_time:.2f} seconds.\")\n",
    "\n",
    "    # LSH-based K-NN\n",
    "    print(\"Performing LSH-based K-NN...\")\n",
    "    threshold=0.4\n",
    "    for num_perm in [16,32,64]:\n",
    "\n",
    "        start_time = time()\n",
    "        lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n",
    "        train_minhashes = [create_minhash(list_of_train_shingle[traindex], num_perm) for traindex in range(len(list_of_train_shingle))]\n",
    "        \n",
    "        for i, minhash in enumerate(train_minhashes):\n",
    "            lsh.insert(str(i), minhash)\n",
    "        \n",
    "        build_time = time() - start_time\n",
    "        start_time = time()\n",
    "        lsh_results = []\n",
    "        \n",
    "        for teindex in range(len(list_of_test_shingle)):\n",
    "            test_minhash = create_minhash(list_of_test_shingle[teindex], num_perm)\n",
    "            candidates = lsh.query(test_minhash)\n",
    "            lsh_results.append([int(cand) for cand in candidates])\n",
    "        \n",
    "        query_time = time() - start_time\n",
    "        \n",
    "        matched_fractions = []\n",
    "        for brute_neighbors, lsh_neighbors in zip(nearest_neighbors, lsh_results):\n",
    "            brute_set=set(brute_neighbors)\n",
    "            lsh_set=set(lsh_neighbors)\n",
    "            matched_count = len(brute_set.intersection(lsh_set))\n",
    "            matched_fraction = matched_count / len(brute_set) if brute_set else 0\n",
    "            matched_fractions.append(matched_fraction)\n",
    "        \n",
    "        average_fraction = np.mean(matched_fractions) if matched_fractions else 0\n",
    "        \n",
    "        print(f\"LSH Results (num_perm={num_perm},threshold={threshold}):\")\n",
    "        print(f\"  Build Time: {build_time:.2f} seconds\")\n",
    "        print(f\"  Query Time: {query_time:.2f} seconds\")\n",
    "        print(f\"  Fraction Matched: {average_fraction:.2f}\")\n",
    "if __name__ == \"__main__\":\n",
    "    nearest_neighbor_search()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
