{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from time import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "\n",
    "# If you have never dowloaded the stopwords package from ntlk before, you need to uncomment and run the next two lines\n",
    "# import nltk\n",
    "# nltk.download(\"stopwords\")\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing,removing punctuation and removing stopwords\n",
    "    :param text: The input text string.\n",
    "    :return: Preprocessed text string.\n",
    "    \"\"\"\n",
    "\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
    "    text = re.sub(r'[^\\w]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "def text_to_shingles(text, k=3):\n",
    "    \"\"\"\n",
    "    Convert text into shingles\n",
    "    :param text: Input string to be converted into shingles.\n",
    "    :param k: Shingle size (default is 3).\n",
    "    :return: A set of k-length shingles.\n",
    "    \"\"\"\n",
    "    text = preprocess_text(text)  # Preprocess the text\n",
    "\n",
    "    shingles = []\n",
    "    for i in range(0, len(text) - k):\n",
    "        shingles.append(text[i:i + k])\n",
    "    return set(shingles)\n",
    "\n",
    "\n",
    "def jaccard_similarity(doc1: set,doc2:set):\n",
    "    intersection = doc1.intersection(doc2)\n",
    "    union = doc1.union(doc2)\n",
    "\n",
    "    return 1 - len(intersection)/len(union)\n",
    "\n",
    "\n",
    "def create_minhash(shingle_doc, num_perm):\n",
    "    \"\"\"\n",
    "    Create an min_hash index for the doc shingle_doc.\n",
    "    The min_hash index is initialized with num_perm permutations\n",
    "    We are basically trying to simulate the example found in the datasketch documentation site\n",
    "    \"\"\"\n",
    "\n",
    "    m = MinHash(num_perm=num_perm)\n",
    "\n",
    "    for shingle in shingle_doc:\n",
    "        m.update(shingle.encode('utf8'))\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "print(\"Loading the training and the test datasets...\")\n",
    "train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "fraction=0.01 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "# Split the training dataset into a smaller one\n",
    "print(f\"Splitting the training dataset to a smaller one (fraction:{fraction*100}%) ...\")\n",
    "train_subset, _ = train_test_split(\n",
    "    train_ds_comma, \n",
    "    train_size=fraction, \n",
    "    stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "    random_state=42 \n",
    ")\n",
    "\n",
    "train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "# Split the testing dataset into a smaller one\n",
    "print(f\"Splitting the testing dataset to a smaller one (fraction:{fraction*100}%) ...\")\n",
    "test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "# the two sets that we are going to work on\n",
    "train_subset=pd.read_csv('train_subset.csv')\n",
    "test_subset=pd.read_csv('test_subset.csv')  \n",
    "\n",
    "train_df = train_subset\n",
    "test_df = test_subset\n",
    "\n",
    "# Example dataset\n",
    "train_docs = train_df['Content'] + train_df['Title']\n",
    "test_docs = test_df['Content'] + test_df['Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing of the text data\n",
    "print(\"Performing preprocessing and shingling of the data...\")\n",
    "\n",
    "k_shingle=3\n",
    "\n",
    "print(f\"Shingling the training set... (k={k_shingle})\")\n",
    "list_of_train_shingle=[]\n",
    "for tr_doc in train_docs:\n",
    "    sh_doc=text_to_shingles(tr_doc,k=k_shingle)\n",
    "    list_of_train_shingle.append(sh_doc)\n",
    "\n",
    "print(f\"Shingling the test set... (k={k_shingle})\")\n",
    "\n",
    "list_of_test_shingle=[]\n",
    "for te_doc in test_docs:\n",
    "    sh_doc=text_to_shingles(te_doc,k=k_shingle)\n",
    "    list_of_test_shingle.append(sh_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Part 2: Nearest Neighbor Search with Locality Sensitive Hashing (LSH)\n",
    "    \n",
    "# Compute K=7 nearest neighbors for each test document\n",
    "K = 7\n",
    "nearest_neighbors = []\n",
    "    \n",
    "# Brute-force K-NN\n",
    "print(\"Performing Brute-Force K-NN...\")\n",
    "start_time = time() # we start the clock\n",
    "    \n",
    "for teindex in range(len(list_of_test_shingle)):\n",
    "    similarities = []\n",
    "    \n",
    "    # for every doc in the test set, we compute the Jaccard with every doc from the train set\n",
    "    for traindex in range(len(list_of_train_shingle)):\n",
    "        sim = jaccard_similarity(list_of_test_shingle[teindex], list_of_train_shingle[traindex])\n",
    "        similarities.append((traindex, sim)) # we keep indexes of train sets\n",
    "    \n",
    "    # we sort the similiraties/distances in ascending order\n",
    "    sorted_sim=sorted(similarities,key=lambda x : x[1])\n",
    "    top_k_neighbors = [idx for idx, sim in sorted_sim[:K]]  # and we keep the best K out of them\n",
    "        \n",
    "    nearest_neighbors.append(top_k_neighbors)\n",
    "    \n",
    "brute_force_time = time() - start_time # we stop the clock\n",
    "\n",
    "\n",
    "print(f\"Brute-Force K-NN completed in {brute_force_time:.2f} seconds.\")\n",
    "\n",
    "# LSH-based K-NN\n",
    "print(\"Performing LSH-based K-NN...\")\n",
    "threshold=0.4 # the threshold of similarity used on LSH\n",
    "# we try different number of permutations, per the instructions\n",
    "for num_perm in [16,32,64]:\n",
    "\n",
    "    start_time = time() # we start the clock\n",
    "    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm) # we create the LSH table\n",
    "    train_minhashes = [create_minhash(list_of_train_shingle[traindex], num_perm) for traindex in range(len(list_of_train_shingle))] # we create the lsh indexes\n",
    "        \n",
    "    for i, minhash in enumerate(train_minhashes):\n",
    "        lsh.insert(str(i), minhash)\n",
    "        \n",
    "    build_time = time() - start_time # we stop the clock, LSH has been built\n",
    "    start_time = time() # we start the clock\n",
    "    lsh_results = []\n",
    "\n",
    "    # we perform the queries for all the test docs  \n",
    "    for teindex in range(len(list_of_test_shingle)):\n",
    "        test_minhash = create_minhash(list_of_test_shingle[teindex], num_perm) # we create the lsh index for the query\n",
    "        candidates = lsh.query(test_minhash) # we query it\n",
    "        lsh_results.append([int(cand) for cand in candidates]) # we get the answer\n",
    "        \n",
    "    query_time = time() - start_time # we stop the clock, all the test queries have been answered\n",
    "        \n",
    "    matched_fractions = []\n",
    "    for brute_neighbors, lsh_neighbors in zip(nearest_neighbors, lsh_results):\n",
    "        brute_set=set(brute_neighbors) # the set with the brute force kneighbors\n",
    "        lsh_set=set(lsh_neighbors) # the set with the lsh neighbors\n",
    "        matched_count = len(brute_set.intersection(lsh_set)) # we found the intersection of the brute force with LSH\n",
    "        matched_fraction = matched_count / len(brute_set) if brute_set else 0 # we calculate the faction if the brute force has returned, else we get 0\n",
    "        matched_fractions.append(matched_fraction)\n",
    "        \n",
    "    average_fraction = np.mean(matched_fractions) if matched_fractions else 0 # we average over the fractions we got for all the queries\n",
    "        \n",
    "    print(f\"LSH Results (num_perm={num_perm},threshold={threshold}):\")\n",
    "    print(f\"  Build Time: {build_time:.2f} seconds\")\n",
    "    print(f\"  Query Time: {query_time:.2f} seconds\")\n",
    "    print(f\"  Fraction Matched: {average_fraction:.2f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
