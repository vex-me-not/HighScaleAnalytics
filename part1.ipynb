{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "\n",
    "\n",
    "from time import time # needed to measure the elapsed time\n",
    "from itertools import zip_longest\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the answer to part 1 of the exercise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the answer to part 1 of the exercise\\n\")\n",
    "\n",
    "# we read the train and the test datasets from the respective files\n",
    "train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "fraction=0.15 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "# Split the training dataset into a smaller one\n",
    "train_subset, _ = train_test_split(\n",
    "    train_ds_comma, \n",
    "    train_size=fraction, \n",
    "    stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "# Split the testing dataset into a smaller one\n",
    "test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "# the two sets that we are going to work on\n",
    "train_subset=pd.read_csv('train_subset.csv')\n",
    "test_subset=pd.read_csv('test_subset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Random Forest\n",
      "\n",
      "Testing: SVM\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, tokenization and removal of stop words via Vectorizer() of sklearn\n",
    "# This process is called vectorization, in which documents are transformed into numerical represantations\n",
    "# It was observed that TfidVectorizer works better in combination with LinearSVC, which is the recommended SVM method for large number of samples \n",
    "\n",
    "vctrz= TfidfVectorizer(stop_words='english')\n",
    "x_train=vctrz.fit_transform(train_subset['Title']+train_subset['Content'])\n",
    "y_train=train_subset['Label']\n",
    "x_test=vctrz.transform(test_subset['Title']+ test_subset['Content'])\n",
    "\n",
    "# The classifiers we are going to compare\n",
    "classifiers={'Random Forest': RandomForestClassifier(),\n",
    "            #  'SVM': SVC(),\n",
    "             'SVM':LinearSVC()}\n",
    "\n",
    "# the results of the comparison\n",
    "results={}\n",
    "\n",
    "# we iterate over the set of the classifiers and we perform cross validation\n",
    "for name,clfr in classifiers.items():\n",
    "    print(\"\\nTesting:\",name)\n",
    "    scores=cross_val_score(clfr,x_train,y_train,cv=5,scoring='accuracy')\n",
    "    results[name]=scores.mean() # the score of the validation of the classifier clfr\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Results\n",
      "  ->Random Forest: 90.33334275893871 %\n",
      "  ->SVM: 95.85547416930626 %\n",
      "The best method is: SVM\n"
     ]
    }
   ],
   "source": [
    "print(\"5-Fold Cross-Validation Results\")\n",
    "for name,res in results.items():\n",
    "    print(f\"  ->{name}: {res*100} %\")\n",
    "\n",
    "# we sort the results we got in descending order, so that we can get the best method\n",
    "sorted_results=dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "\n",
    "# the best method, according to the 5-Fold Validation we performed\n",
    "best_fit_name= list(sorted_results.keys())[0]\n",
    "print(\"The best method is:\",best_fit_name)\n",
    "\n",
    "# we apply our best method on the training set\n",
    "best_fit_method=classifiers[best_fit_name]\n",
    "best_fit_method.fit(x_train,y_train)\n",
    "\n",
    "# we predict on the test set, using our best method\n",
    "test_subset['Predicted'] = best_fit_method.predict(x_test)\n",
    "test_subset[['Id', 'Predicted']].to_csv('testSet_categories.csv', index=False) # we output the predictions in the respective .csv fil\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
