{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from time import time # needed to measure the elapsed time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_distance(vector1:csr_matrix, vector2:csr_matrix):\n",
    "    \"\"\"\n",
    "    Compute Jaccard distance between two sparse vectors/matrices.\n",
    "    :vector1: First sparse vector, containing float numbers.\n",
    "    :vector2: Second sparse vector, containing float numbers\n",
    "    :return: Jaccard distance score(1-Jaccard similarity).\n",
    "    \"\"\"\n",
    "    # we make the vectors/matrices dense so that we can work on them\n",
    "    v1dense=vector1.todense()\n",
    "    v2dense=vector2.todense()\n",
    "    \n",
    "    # we turn them into boolean vectors, in which every value is either True or False\n",
    "    v1bool=v1dense.astype(bool)\n",
    "    v2bool=v2dense.astype(bool)\n",
    "\n",
    "    # The intersection of the two boolean vectors is the result of the logical AND\n",
    "    intersection = np.logical_and(v1bool,v2bool) \n",
    "    \n",
    "    # The union of the two boolean vectors is the result of the logical OR\n",
    "    union = np.logical_or(v1bool,v2bool)\n",
    "    \n",
    "    # The size of the intersection is the number of all the non-Zero(non-False) items\n",
    "    intersection_size=np.count_nonzero(intersection)\n",
    "    # The size of the union is the number of all the non-Zero(non-False) items\n",
    "    union_size=np.count_nonzero(union)\n",
    "\n",
    "    return (1- intersection_size / union_size) if union_size > 0 else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the answer to part 1 of the exercise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the answer to part 1 of the exercise\\n\")\n",
    "\n",
    "# we read the train and the test datasets from the respective files\n",
    "train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "fraction=0.15 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "# Split the training dataset into a smaller one\n",
    "train_subset, _ = train_test_split(\n",
    "    train_ds_comma, \n",
    "    train_size=fraction, \n",
    "    stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "# Split the testing dataset into a smaller one\n",
    "test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "# the two sets that we are going to work on\n",
    "train_subset=pd.read_csv('train_subset.csv')\n",
    "test_subset=pd.read_csv('test_subset.csv')\n",
    "\n",
    "\n",
    "# If you don't want to run for the complete dataset, but just for subsets, comment the next two lines\n",
    "train_subset=train_ds_comma\n",
    "test_subset=test_ds_comma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: Random Forest\n",
      "\n",
      "Testing: SVM\n",
      "\n",
      "Testing: kNN\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, tokenization and removal of stop words via Vectorizer() of sklearn\n",
    "# This process is called vectorization, in which documents are transformed into numerical represantations\n",
    "# It was observed that TfidVectorizer works better in combination with LinearSVC, which is the recommended SVM method for large number of samples \n",
    "\n",
    "vctrz= TfidfVectorizer(stop_words='english')\n",
    "x_train=vctrz.fit_transform(train_subset['Title']+train_subset['Content'])\n",
    "y_train=train_subset['Label']\n",
    "x_test=vctrz.transform(test_subset['Title']+ test_subset['Content'])\n",
    "\n",
    "# The classifiers we are going to compare\n",
    "classifiers={'Random Forest': RandomForestClassifier(),\n",
    "             'SVM':LinearSVC()\n",
    "             }\n",
    "# We have discovered that kNN is too slow, even for vectorized operations. With that in mind and with some a posteriori knowledge that can be\n",
    "# found in the report , we have decided to include it only if the size of the subsets is relatively low.\n",
    "if(fraction<0.05):\n",
    "    classifiers[\"kNN\"]=KNeighborsClassifier(n_neighbors=7,algorithm='brute',metric=jaccard_distance)\n",
    "\n",
    "# the results of the comparison\n",
    "results={}\n",
    "times={}\n",
    "# we iterate over the set of the classifiers and we perform cross validation\n",
    "for name,clfr in classifiers.items():\n",
    "    print(\"\\nTesting:\",name)\n",
    "    start_time = time()\n",
    "    scores=cross_val_score(clfr,x_train,y_train,cv=5,scoring='accuracy')\n",
    "    results[name]=scores.mean() # the score of the validation of the classifier clfr\n",
    "    times[name]=time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Results for 1.00% of the two sets\n",
      "  ->SVM: 90.06% (Runtime  0.15 secs)\n",
      "  ->kNN: 85.14% (Runtime  275.00 secs)\n",
      "  ->Random Forest: 83.08% (Runtime  6.54 secs)\n",
      "The best method is: SVM\n"
     ]
    }
   ],
   "source": [
    "# we sort the results we got in descending order, so that we can get the best method\n",
    "sorted_results=dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "    \n",
    "print(f\"5-Fold Cross-Validation Results for{fraction*100: .2f}% of the two sets\")\n",
    "for name,res in sorted_results.items():\n",
    "    print(f\"  ->{name}:{res*100: .2f}% (Runtime {times[name]: .2f} secs)\")\n",
    "\n",
    "\n",
    "# the best method, according to the 5-Fold Validation we performed\n",
    "best_fit_name= list(sorted_results.keys())[0]\n",
    "print(\"The best method, that we will use to predict, is:\",best_fit_name)\n",
    "\n",
    "# we apply our best method on the training set\n",
    "best_fit_method=classifiers[best_fit_name]\n",
    "best_fit_method.fit(x_train,y_train)\n",
    "\n",
    "# we predict on the test set, using our best method\n",
    "test_subset['Predicted'] = best_fit_method.predict(x_test)\n",
    "test_subset[['Id', 'Predicted']].to_csv('testSet_categories.csv', index=False) # we output the predictions in the respective .csv fil\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
