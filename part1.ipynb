{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, HashingVectorizer\n",
    "from sklearn.model_selection import cross_val_score,StratifiedKFold\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "from time import time # needed to measure the elapsed time\n",
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_shingles(text, k=2):\n",
    "    words = re.split(r'[,.\\s!\\n!\"?;_:*@]',text)\n",
    "    shingles = set(zip_longest(*[words[i:] for i in range(k)], fillvalue=''))\n",
    "    # Filter out incomplete shingles containing the fillvalue\n",
    "    return {shingle for shingle in shingles if '' not in shingle}\n",
    "\n",
    "# this yields better results with high ks\n",
    "# def text_to_shingles(text, k=5):\n",
    "#     doc= re.sub('[,;.:?!@*&# ]','',text)\n",
    "#     shingles = []\n",
    "#     for i in range(0, len(doc) - k):\n",
    "#         shingles.append(doc[i:i + k])\n",
    "#     return set(shingles)\n",
    "\n",
    "# def text_to_shingles(text,k=2):\n",
    "#     words=text.lower()\n",
    "#     return set(re.split(r'[,.\\s!\"?;_*@]',words))\n",
    "\n",
    "\n",
    "def jaccard_distance(vector1, vector2):\n",
    "    \"\"\"\n",
    "    Compute Jaccard distance between two sparse vectors/matrices.\n",
    "    :vector1: First sparse vector, containing float numbers.\n",
    "    :vector2: Second sparse vector, containing float numbers\n",
    "    :return: Jaccard distance score(1-Jaccard similarity).\n",
    "    \"\"\"\n",
    "    # we make the vectors/matrices dense so that we can work on them\n",
    "    v1dense=vector1.todense()\n",
    "    v2dense=vector2.todense()\n",
    "    \n",
    "    # we turn them into boolean vectors, in which every value is either True or False\n",
    "    v1bool=v1dense.astype(bool)\n",
    "    v2bool=v2dense.astype(bool)\n",
    "\n",
    "    # The intersection of the two boolean vectors is the result of the logical AND\n",
    "    intersection = np.logical_and(v1bool,v2bool) \n",
    "    \n",
    "    # The union of the two boolean vectors is the result of the logical OR\n",
    "    union = np.logical_or(v1bool,v2bool)\n",
    "    \n",
    "    # The size of the intersection is the number of all the non-Zero(non-False) items\n",
    "    intersection_size=np.count_nonzero(intersection)\n",
    "    # The size of the union is the number of all the non-Zero(non-False) items\n",
    "    union_size=np.count_nonzero(union)\n",
    "\n",
    "    return (1- intersection_size / union_size) if union_size > 0 else 0\n",
    "\n",
    "# Jaccard similarity for sets\n",
    "def jaccard_similarity_set(set1, set2):\n",
    "    inter=0\n",
    "    union=len(set1)+len(set2)\n",
    "    for item in set1:\n",
    "        if item in set2:\n",
    "            inter +=1\n",
    "            union -=1\n",
    "    return inter / union\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by lowercasing and removing punctuation.\n",
    "    :param text: The input text string.\n",
    "    :return: Preprocessed text string.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = re.sub('[.,!?:@%*()]', ' ', text)  # Remove punctuation\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the answer to part 1 of the exercise\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"This is the answer to part 1 of the exercise\\n\")\n",
    "\n",
    "# we read the train and the test datasets from the respective files\n",
    "train_ds_comma=pd.read_csv(\"train.csv\")\n",
    "test_ds_comma=pd.read_csv(\"test_without_labels.csv\")\n",
    "\n",
    "\n",
    "fraction=0.50 # the fraction of the datasets, used to create smaller, faster to work on datasets\n",
    "\n",
    "# Split the training dataset into a smaller one\n",
    "train_subset, _ = train_test_split(\n",
    "    train_ds_comma, \n",
    "    train_size=fraction, \n",
    "    stratify=train_ds_comma['Label'],  # Ensure stratified sampling\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_subset.to_csv('train_subset.csv', index=False)\n",
    "\n",
    "# Split the testing dataset into a smaller one\n",
    "test_subset=test_ds_comma.sample(frac=fraction,random_state=42)\n",
    "test_subset.to_csv('test_subset.csv',index=False)\n",
    "\n",
    "# the two sets that we are going to work on\n",
    "train_subset=pd.read_csv('train_subset.csv')\n",
    "test_subset=pd.read_csv('test_subset.csv')\n",
    "\n",
    "train_subset=train_ds_comma\n",
    "test_subset=test_ds_comma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing: SVM\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing, tokenization and removal of stop words via Vectorizer() of sklearn\n",
    "# This process is called vectorization, in which documents are transformed into numerical represantations\n",
    "# It was observed that TfidVectorizer works better in combination with LinearSVC, which is the recommended SVM method for large number of samples \n",
    "\n",
    "vctrz= TfidfVectorizer(stop_words='english')\n",
    "# x_train=vctrz.fit_transform(train_subset['Title'].apply(preprocess_text)+train_subset['Content'].apply(preprocess_text))\n",
    "x_train=vctrz.fit_transform(train_subset['Title']+train_subset['Content'])\n",
    "# print(x_train.toarray())\n",
    "y_train=train_subset['Label']\n",
    "# x_test=vctrz.transform(test_subset['Title'].apply(preprocess_text)+ test_subset['Content'].apply(preprocess_text))\n",
    "x_test=vctrz.transform(test_subset['Title']+ test_subset['Content'])\n",
    "# The classifiers we are going to compare\n",
    "classifiers={'Random Forest': RandomForestClassifier(),\n",
    "             'SVM':LinearSVC()\n",
    "             }\n",
    "if(fraction<0.10):\n",
    "    classifiers[\"kNN\"]=KNeighborsClassifier(n_neighbors=7,algorithm='brute',metric=jaccard_distance)\n",
    "\n",
    "# the results of the comparison\n",
    "results={}\n",
    "times={}\n",
    "# we iterate over the set of the classifiers and we perform cross validation\n",
    "for name,clfr in classifiers.items():\n",
    "    print(\"\\nTesting:\",name)\n",
    "    start_time = time()\n",
    "    scores=cross_val_score(clfr,x_train,y_train,cv=5,scoring='accuracy')\n",
    "    results[name]=scores.mean() # the score of the validation of the classifier clfr\n",
    "    times[name]=time() - start_time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5-Fold cross validation of KNN\n",
    "# if(fraction<=0.01 and 0):\n",
    "#     skf=StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     X=train_subset['Title'].apply(preprocess_text)+train_subset['Content'].apply(preprocess_text)\n",
    "#     y=train_subset['Label']\n",
    "\n",
    "#     accur=[]\n",
    "\n",
    "\n",
    "#     for train_index, val_index in skf.split(X, y):\n",
    "#         # Split into train and validation sets\n",
    "#         X_train_knn, X_val_knn = X.iloc[train_index], X.iloc[val_index]\n",
    "#         y_train_knn, y_val_knn = y.iloc[train_index], y.iloc[val_index]\n",
    "\n",
    "\n",
    "\n",
    "#         train_shingles = X_train_knn.apply(text_to_shingles)\n",
    "#         val_shingles = X_val_knn.apply(text_to_shingles)\n",
    "\n",
    "#         # print(train_shingles)\n",
    "#         # print(val_shingles)\n",
    "#         # Perform brute-force K-NN for validation\n",
    "#         predictions = []\n",
    "#         k=7\n",
    "#         for val_doc in val_shingles:\n",
    "#             similarities = [(i, jaccard_similarity(val_doc,train_doc))\n",
    "#                                 for i, train_doc in enumerate(train_shingles)]\n",
    "#             similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "#             # print(similarities)\n",
    "#             # Majority voting\n",
    "#             neighbor_labels = [y_train_knn.iloc[idx] for idx, _ in similarities]\n",
    "#             majority_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "#             predictions.append(majority_label)\n",
    "\n",
    "#         # Compute accuracy\n",
    "#         accuracy = accuracy_score(y_val_knn, predictions)\n",
    "#         accur.append(accuracy)\n",
    "\n",
    "#     knn_acc=np.mean(accur)\n",
    "# else:\n",
    "#     knn_acc=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-Fold Cross-Validation Results for 100.00% of the two sets\n",
      "  ->SVM: 97.42% (Runtime  23.39 secs)\n",
      "The best method is: SVM\n"
     ]
    }
   ],
   "source": [
    "# results['kNN']=knn_acc\n",
    "sorted_results=dict(sorted(results.items(), key=lambda item: item[1],reverse=True))\n",
    "    \n",
    "print(f\"5-Fold Cross-Validation Results for{fraction*100: .2f}% of the two sets\")\n",
    "for name,res in sorted_results.items():\n",
    "    print(f\"  ->{name}:{res*100: .2f}% (Runtime {times[name]: .2f} secs)\")\n",
    "\n",
    "# we sort the results we got in descending order, so that we can get the best method\n",
    "\n",
    "\n",
    "# the best method, according to the 5-Fold Validation we performed\n",
    "# best_fit_name='SVM'\n",
    "\n",
    "best_fit_name= list(sorted_results.keys())[0]\n",
    "print(\"The best method is:\",best_fit_name)\n",
    "\n",
    "# we apply our best method on the training set\n",
    "best_fit_method=classifiers[best_fit_name]\n",
    "best_fit_method.fit(x_train,y_train)\n",
    "\n",
    "# we predict on the test set, using our best method\n",
    "test_subset['Predicted'] = best_fit_method.predict(x_test)\n",
    "test_subset[['Id', 'Predicted']].to_csv('testSet_categories.csv', index=False) # we output the predictions in the respective .csv fil\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
