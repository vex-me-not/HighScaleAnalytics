{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from time import time\n",
    "from itertools import zip_longest\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "# Part 2: Nearest Neighbor Search with LSH\n",
    "def nearest_neighbor_search():\n",
    "    train_df = pd.read_csv('train_set.csv', sep='|')\n",
    "    test_df = pd.read_csv('test_set.csv', sep='|')\n",
    "\n",
    "    # Convert text to shingles (sets of words)\n",
    "    def text_to_shingles(text, k=5):\n",
    "        words = text.split()\n",
    "        return set(zip_longest(*[words[i:] for i in range(k)], fillvalue=''))\n",
    "\n",
    "    # Create MinHash signatures\n",
    "    def create_minhash(text, num_perm):\n",
    "        m = MinHash(num_perm=num_perm)\n",
    "        for shingle in text_to_shingles(text):\n",
    "            m.update(' '.join(shingle).encode('utf-8'))\n",
    "        return m\n",
    "\n",
    "    # Brute-force K-NN using Jaccard similarity\n",
    "    start_time = time()\n",
    "    train_shingles = train_df['Content'].apply(text_to_shingles)\n",
    "    test_shingles = test_df['Content'].apply(text_to_shingles)\n",
    "\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        return len(set1 & set2) / len(set1 | set2)\n",
    "\n",
    "    k = 7  # Number of nearest neighbors\n",
    "    brute_force_results = []\n",
    "    brute_force_predictions = []\n",
    "    for test_doc in test_shingles:\n",
    "        similarities = [(i, jaccard_similarity(test_doc, train_doc)) for i, train_doc in enumerate(train_shingles)]\n",
    "        similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
    "        brute_force_results.append([idx for idx, _ in similarities])\n",
    "\n",
    "        # Majority voting for classification\n",
    "        neighbor_labels = [train_df.iloc[idx]['Label'] for idx, _ in similarities]\n",
    "        majority_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "        brute_force_predictions.append(majority_label)\n",
    "\n",
    "    brute_force_time = time() - start_time\n",
    "    print(f\"Brute-Force K-NN Time: {brute_force_time}\")\n",
    "\n",
    "    test_df['Predicted_BruteForce'] = brute_force_predictions\n",
    "    test_df[['Id', 'Predicted_BruteForce']].to_csv('brute_force_knn_predictions.csv', index=False)\n",
    "\n",
    "    # LSH with Min-Hashing\n",
    "    for num_perm in [16, 32, 64]:\n",
    "        start_time = time()\n",
    "        lsh = MinHashLSH(threshold=0.9, num_perm=num_perm)\n",
    "\n",
    "        for i, text in enumerate(train_df['Content']):\n",
    "            lsh.insert(i, create_minhash(text, num_perm))\n",
    "\n",
    "        build_time = time() - start_time\n",
    "\n",
    "        start_time = time()\n",
    "        lsh_results = []\n",
    "        lsh_predictions = []\n",
    "        for test_idx, test_text in enumerate(test_df['Content']):\n",
    "            minhash_test = create_minhash(test_text, num_perm)\n",
    "            candidates = lsh.query(minhash_test)\n",
    "            similarities = [(idx, jaccard_similarity(text_to_shingles(train_df.loc[idx, 'Content']), text_to_shingles(test_text))) for idx in candidates]\n",
    "            similarities = sorted(similarities, key=lambda x: x[1], reverse=True)[:k]\n",
    "\n",
    "            if similarities:\n",
    "                lsh_results.append([idx for idx, _ in similarities])\n",
    "                neighbor_labels = [train_df.iloc[idx]['Label'] for idx, _ in similarities]\n",
    "                majority_label = Counter(neighbor_labels).most_common(1)[0][0]\n",
    "                lsh_predictions.append(majority_label)\n",
    "            else:\n",
    "                lsh_results.append([])\n",
    "                lsh_predictions.append(\"Unknown\")  # Handle cases where no candidates are found\n",
    "\n",
    "        query_time = time() - start_time\n",
    "\n",
    "        # Calculate fraction of true K-most similar documents returned\n",
    "        matched_fractions = []\n",
    "        for brute_force_neighbors, lsh_neighbors in zip(brute_force_results, lsh_results):\n",
    "            if brute_force_neighbors:\n",
    "                matched_count = len(set(brute_force_neighbors) & set(lsh_neighbors))\n",
    "                matched_fraction = matched_count / len(brute_force_neighbors)\n",
    "                matched_fractions.append(matched_fraction)\n",
    "        average_fraction = np.mean(matched_fractions) if matched_fractions else 0\n",
    "\n",
    "        print(f\"LSH Results (Permutations={num_perm}):\")\n",
    "        print(f\"Build Time: {build_time}, Query Time: {query_time}, Fraction Matched: {average_fraction:.2f}\")\n",
    "\n",
    "        test_df[f'Predicted_LSH_{num_perm}'] = lsh_predictions\n",
    "        test_df[[\"Id\", f\"Predicted_LSH_{num_perm}\"]].to_csv(f'lsh_knn_predictions_{num_perm}.csv', index=False)\n",
    "\n",
    "# Part 3: Dynamic Time Warping\n",
    "def dtw_distance(seq_a, seq_b):\n",
    "    n, m = len(seq_a), len(seq_b)\n",
    "    dtw_matrix = np.full((n + 1, m + 1), float('inf'))\n",
    "    dtw_matrix[0, 0] = 0\n",
    "\n",
    "    for i in range(1, n + 1):\n",
    "        for j in range(1, m + 1):\n",
    "            cost = abs(seq_a[i - 1] - seq_b[j - 1])\n",
    "            dtw_matrix[i, j] = cost + min(dtw_matrix[i - 1, j], dtw_matrix[i, j - 1], dtw_matrix[i - 1, j - 1])\n",
    "\n",
    "    return dtw_matrix[n, m]\n",
    "\n",
    "def dynamic_time_warping():\n",
    "    data = pd.read_csv('time_series.csv')\n",
    "    results = []\n",
    "\n",
    "    start_time = time()\n",
    "    for idx, row in data.iterrows():\n",
    "        seq_a = np.array(eval(row['seq_a']))\n",
    "        seq_b = np.array(eval(row['seq_b']))\n",
    "        dtw_dist = dtw_distance(seq_a, seq_b)\n",
    "        results.append({'id': row['id'], 'DTW distance': dtw_dist})\n",
    "    total_time = time() - start_time\n",
    "\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_csv('dtw.csv', index=False)\n",
    "    print(f\"Total DTW Time: {total_time}\")\n",
    "\n",
    "# Execute tasks\n",
    "if __name__ == \"__main__\":\n",
    "    text_classification()\n",
    "    nearest_neighbor_search()\n",
    "    dynamic_time_warping()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
